{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "partial-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDeepInsight import ImageTransformer, LogScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy import linalg as scipy_linalg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, plot_confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exempt-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "prosody_data = pd.read_csv('Prosodic_Features_MRH.csv',sep=\",\")\n",
    "#expr = pd.read_csv(expr_file, sep=\"\\t\")\n",
    "y = prosody_data['Hired3C'].values\n",
    "#36\n",
    "#X =prosody_data[['duration', 'energy', 'power', 'min_pitch', 'max_pitch', 'mean_pitch', 'pitch_sd', 'pitch_abs', 'pitch_quant', 'pitchUvsVRatio', 'Time:8', 'iDifference', 'diffPitchMaxMin', 'diffPitchMaxMean', 'diffPitchMaxMode', 'intensityMin', 'intensityMax', 'intensityMean', 'intensitySD', 'intensityQuant', 'diffIntMaxMin', 'diffIntMaxMean', 'diffIntMaxMode', 'avgVal1', 'avgVal2', 'avgVal3', 'avgBand1', 'avgBand2', 'avgBand3', 'fmean1', 'fmean2', 'fmean3', 'f2meanf1', 'f3meanf1', 'f1STD', 'f2STD']]\n",
    "#49X =prosody_data[['duration', 'energy', 'power', 'min_pitch', 'max_pitch', 'mean_pitch', 'pitch_sd', 'pitch_abs', 'pitch_quant', 'pitchUvsVRatio', 'Time:8', 'iDifference', 'diffPitchMaxMin', 'diffPitchMaxMean', 'diffPitchMaxMode', 'intensityMin', 'intensityMax', 'intensityMean', 'intensitySD', 'intensityQuant', 'diffIntMaxMin', 'diffIntMaxMean', 'diffIntMaxMode', 'avgVal1', 'avgVal2', 'avgVal3', 'avgBand1', 'avgBand2', 'avgBand3', 'fmean1', 'fmean2', 'fmean3', 'f2meanf1', 'f3meanf1', 'f1STD', 'f2STD', 'f3STD', 'f2STDf1', 'f2STDf2', 'jitter', 'shimmer', 'jitterRap', 'meanPeriod', 'percentUnvoiced', 'numVoiceBreaks', 'PercentBreaks', 'speakRate', 'numPause', 'maxDurPause']]\n",
    "#25X = prosody_data[['pitch_quant', 'meanPeriod', 'mean_pitch', 'fmean3', 'f3STD', 'f2STD', 'pitch_sd', 'shimmer', 'avgBand2', 'intensityMean', 'jitter', 'intensitySD', 'intensityQuant', 'jitterRap', 'f1STD', 'avgVal2', 'fmean2', 'PercentBreaks', 'AvgTotFall:3', 'f2STDf1', 'pitch_abs', 'f2STDf2', 'avgBand1', 'percentUnvoiced', 'AvgTotRis:3']]\n",
    "X = prosody_data.iloc[:, 6:].values\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "#X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intermediate-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC16</th>\n",
       "      <th>PC17</th>\n",
       "      <th>PC18</th>\n",
       "      <th>PC19</th>\n",
       "      <th>PC20</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.692918</td>\n",
       "      <td>0.685098</td>\n",
       "      <td>-0.364534</td>\n",
       "      <td>-0.295984</td>\n",
       "      <td>-0.199671</td>\n",
       "      <td>-0.119706</td>\n",
       "      <td>-0.064272</td>\n",
       "      <td>0.107715</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.027896</td>\n",
       "      <td>-0.120267</td>\n",
       "      <td>-0.123190</td>\n",
       "      <td>-0.279014</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>-0.031081</td>\n",
       "      <td>0.040511</td>\n",
       "      <td>-0.006124</td>\n",
       "      <td>-0.035574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.753818</td>\n",
       "      <td>0.647446</td>\n",
       "      <td>-0.444903</td>\n",
       "      <td>-0.416003</td>\n",
       "      <td>-0.273987</td>\n",
       "      <td>-0.100499</td>\n",
       "      <td>-0.143152</td>\n",
       "      <td>0.152431</td>\n",
       "      <td>-0.033701</td>\n",
       "      <td>0.082542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051552</td>\n",
       "      <td>0.128479</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>-0.144448</td>\n",
       "      <td>-0.251815</td>\n",
       "      <td>0.100197</td>\n",
       "      <td>-0.039564</td>\n",
       "      <td>0.097527</td>\n",
       "      <td>-0.033825</td>\n",
       "      <td>0.016212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.655107</td>\n",
       "      <td>0.681262</td>\n",
       "      <td>-0.289378</td>\n",
       "      <td>-0.299772</td>\n",
       "      <td>0.038593</td>\n",
       "      <td>-0.109086</td>\n",
       "      <td>-0.244566</td>\n",
       "      <td>0.177346</td>\n",
       "      <td>-0.090378</td>\n",
       "      <td>0.015818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038732</td>\n",
       "      <td>0.245304</td>\n",
       "      <td>-0.059516</td>\n",
       "      <td>-0.255164</td>\n",
       "      <td>-0.140905</td>\n",
       "      <td>-0.003871</td>\n",
       "      <td>0.092548</td>\n",
       "      <td>0.113161</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.089075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.435551</td>\n",
       "      <td>-0.059835</td>\n",
       "      <td>0.440370</td>\n",
       "      <td>-0.898822</td>\n",
       "      <td>0.513097</td>\n",
       "      <td>-0.349614</td>\n",
       "      <td>-0.591225</td>\n",
       "      <td>-0.109355</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>0.045074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071321</td>\n",
       "      <td>0.350818</td>\n",
       "      <td>-0.113005</td>\n",
       "      <td>-0.045958</td>\n",
       "      <td>0.043120</td>\n",
       "      <td>0.189523</td>\n",
       "      <td>0.014996</td>\n",
       "      <td>0.132203</td>\n",
       "      <td>-0.115639</td>\n",
       "      <td>0.005501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.413051</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>0.509778</td>\n",
       "      <td>-0.742404</td>\n",
       "      <td>0.208906</td>\n",
       "      <td>-0.287548</td>\n",
       "      <td>-0.517068</td>\n",
       "      <td>-0.135435</td>\n",
       "      <td>0.177530</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039172</td>\n",
       "      <td>0.261332</td>\n",
       "      <td>-0.058636</td>\n",
       "      <td>-0.057109</td>\n",
       "      <td>0.020498</td>\n",
       "      <td>0.156747</td>\n",
       "      <td>-0.016800</td>\n",
       "      <td>0.138250</td>\n",
       "      <td>-0.179067</td>\n",
       "      <td>0.093943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0.810759</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>-0.641180</td>\n",
       "      <td>-0.734969</td>\n",
       "      <td>0.215605</td>\n",
       "      <td>0.102423</td>\n",
       "      <td>0.030575</td>\n",
       "      <td>-0.062887</td>\n",
       "      <td>-0.215410</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080727</td>\n",
       "      <td>0.178152</td>\n",
       "      <td>-0.088722</td>\n",
       "      <td>0.041802</td>\n",
       "      <td>-0.085061</td>\n",
       "      <td>-0.032335</td>\n",
       "      <td>-0.062554</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>0.111075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0.766963</td>\n",
       "      <td>0.434614</td>\n",
       "      <td>-0.795144</td>\n",
       "      <td>-0.535864</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>0.216450</td>\n",
       "      <td>0.031868</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>-0.196559</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013980</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>0.122430</td>\n",
       "      <td>-0.140203</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>-0.008898</td>\n",
       "      <td>-0.021138</td>\n",
       "      <td>0.206536</td>\n",
       "      <td>0.029052</td>\n",
       "      <td>0.144971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.123206</td>\n",
       "      <td>-0.531445</td>\n",
       "      <td>-0.870132</td>\n",
       "      <td>0.426856</td>\n",
       "      <td>-0.102013</td>\n",
       "      <td>0.108357</td>\n",
       "      <td>-0.377542</td>\n",
       "      <td>-0.122371</td>\n",
       "      <td>-0.283178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046423</td>\n",
       "      <td>0.230259</td>\n",
       "      <td>-0.027055</td>\n",
       "      <td>0.109464</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.226541</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.051280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0.910977</td>\n",
       "      <td>0.535527</td>\n",
       "      <td>-0.670594</td>\n",
       "      <td>-0.157994</td>\n",
       "      <td>0.485442</td>\n",
       "      <td>0.146686</td>\n",
       "      <td>-0.043265</td>\n",
       "      <td>-0.124458</td>\n",
       "      <td>-0.097262</td>\n",
       "      <td>-0.057348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043671</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>-0.080578</td>\n",
       "      <td>-0.165116</td>\n",
       "      <td>0.013067</td>\n",
       "      <td>-0.026326</td>\n",
       "      <td>0.203659</td>\n",
       "      <td>0.071344</td>\n",
       "      <td>0.045116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0.970835</td>\n",
       "      <td>0.345382</td>\n",
       "      <td>-0.327765</td>\n",
       "      <td>-0.459217</td>\n",
       "      <td>0.295382</td>\n",
       "      <td>0.153864</td>\n",
       "      <td>-0.162170</td>\n",
       "      <td>-0.003182</td>\n",
       "      <td>-0.582055</td>\n",
       "      <td>0.204127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.130219</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>0.088295</td>\n",
       "      <td>-0.172877</td>\n",
       "      <td>0.053612</td>\n",
       "      <td>-0.123709</td>\n",
       "      <td>0.163031</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.143555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0   -0.692918  0.685098 -0.364534 -0.295984 -0.199671 -0.119706 -0.064272   \n",
       "1   -0.753818  0.647446 -0.444903 -0.416003 -0.273987 -0.100499 -0.143152   \n",
       "2   -0.655107  0.681262 -0.289378 -0.299772  0.038593 -0.109086 -0.244566   \n",
       "3   -0.435551 -0.059835  0.440370 -0.898822  0.513097 -0.349614 -0.591225   \n",
       "4   -0.413051 -0.004898  0.509778 -0.742404  0.208906 -0.287548 -0.517068   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "675  0.810759  0.264550 -0.641180 -0.734969  0.215605  0.102423  0.030575   \n",
       "676  0.766963  0.434614 -0.795144 -0.535864 -0.001188  0.216450  0.031868   \n",
       "677  0.864723  0.123206 -0.531445 -0.870132  0.426856 -0.102013  0.108357   \n",
       "678  0.910977  0.535527 -0.670594 -0.157994  0.485442  0.146686 -0.043265   \n",
       "679  0.970835  0.345382 -0.327765 -0.459217  0.295382  0.153864 -0.162170   \n",
       "\n",
       "          PC8       PC9      PC10  ...      PC16      PC17      PC18  \\\n",
       "0    0.107715  0.010683  0.011549  ...  0.033984  0.027896 -0.120267   \n",
       "1    0.152431 -0.033701  0.082542  ... -0.051552  0.128479  0.002374   \n",
       "2    0.177346 -0.090378  0.015818  ... -0.038732  0.245304 -0.059516   \n",
       "3   -0.109355  0.091131  0.045074  ... -0.071321  0.350818 -0.113005   \n",
       "4   -0.135435  0.177530  0.000200  ... -0.039172  0.261332 -0.058636   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "675 -0.062887 -0.215410 -0.005058  ...  0.080727  0.178152 -0.088722   \n",
       "676  0.053700 -0.196559  0.013821  ... -0.013980 -0.001574  0.122430   \n",
       "677 -0.377542 -0.122371 -0.283178  ... -0.046423  0.230259 -0.027055   \n",
       "678 -0.124458 -0.097262 -0.057348  ... -0.043671  0.039400  0.021273   \n",
       "679 -0.003182 -0.582055  0.204127  ...  0.053054  0.130219  0.038588   \n",
       "\n",
       "         PC19      PC20      PC21      PC22      PC23      PC24      PC25  \n",
       "0   -0.123190 -0.279014 -0.030345 -0.031081  0.040511 -0.006124 -0.035574  \n",
       "1   -0.144448 -0.251815  0.100197 -0.039564  0.097527 -0.033825  0.016212  \n",
       "2   -0.255164 -0.140905 -0.003871  0.092548  0.113161  0.052736 -0.089075  \n",
       "3   -0.045958  0.043120  0.189523  0.014996  0.132203 -0.115639  0.005501  \n",
       "4   -0.057109  0.020498  0.156747 -0.016800  0.138250 -0.179067  0.093943  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "675  0.041802 -0.085061 -0.032335 -0.062554  0.073359 -0.001346  0.111075  \n",
       "676 -0.140203 -0.142817 -0.008898 -0.021138  0.206536  0.029052  0.144971  \n",
       "677  0.109464 -0.146216  0.038154  0.007096  0.226541  0.008123  0.051280  \n",
       "678 -0.080578 -0.165116  0.013067 -0.026326  0.203659  0.071344  0.045116  \n",
       "679  0.088295 -0.172877  0.053612 -0.123709  0.163031  0.001659  0.143555  \n",
       "\n",
       "[680 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "principalDf = pd.DataFrame(data = X_pca , columns = ['PC1', 'PC2','PC3', 'PC4','PC5', 'PC6','PC7', 'PC8','PC9', 'PC10','PC11', 'PC12','PC13', 'PC14','PC15', 'PC16','PC17', 'PC18','PC19', 'PC20','PC21', 'PC22','PC23', 'PC24','PC25'])\n",
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fancy-tumor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC17</th>\n",
       "      <th>PC18</th>\n",
       "      <th>PC19</th>\n",
       "      <th>PC20</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>Hired3C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.692918</td>\n",
       "      <td>0.685098</td>\n",
       "      <td>-0.364534</td>\n",
       "      <td>-0.295984</td>\n",
       "      <td>-0.199671</td>\n",
       "      <td>-0.119706</td>\n",
       "      <td>-0.064272</td>\n",
       "      <td>0.107715</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027896</td>\n",
       "      <td>-0.120267</td>\n",
       "      <td>-0.123190</td>\n",
       "      <td>-0.279014</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>-0.031081</td>\n",
       "      <td>0.040511</td>\n",
       "      <td>-0.006124</td>\n",
       "      <td>-0.035574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.753818</td>\n",
       "      <td>0.647446</td>\n",
       "      <td>-0.444903</td>\n",
       "      <td>-0.416003</td>\n",
       "      <td>-0.273987</td>\n",
       "      <td>-0.100499</td>\n",
       "      <td>-0.143152</td>\n",
       "      <td>0.152431</td>\n",
       "      <td>-0.033701</td>\n",
       "      <td>0.082542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128479</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>-0.144448</td>\n",
       "      <td>-0.251815</td>\n",
       "      <td>0.100197</td>\n",
       "      <td>-0.039564</td>\n",
       "      <td>0.097527</td>\n",
       "      <td>-0.033825</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.655107</td>\n",
       "      <td>0.681262</td>\n",
       "      <td>-0.289378</td>\n",
       "      <td>-0.299772</td>\n",
       "      <td>0.038593</td>\n",
       "      <td>-0.109086</td>\n",
       "      <td>-0.244566</td>\n",
       "      <td>0.177346</td>\n",
       "      <td>-0.090378</td>\n",
       "      <td>0.015818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245304</td>\n",
       "      <td>-0.059516</td>\n",
       "      <td>-0.255164</td>\n",
       "      <td>-0.140905</td>\n",
       "      <td>-0.003871</td>\n",
       "      <td>0.092548</td>\n",
       "      <td>0.113161</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.089075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.435551</td>\n",
       "      <td>-0.059835</td>\n",
       "      <td>0.440370</td>\n",
       "      <td>-0.898822</td>\n",
       "      <td>0.513097</td>\n",
       "      <td>-0.349614</td>\n",
       "      <td>-0.591225</td>\n",
       "      <td>-0.109355</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>0.045074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350818</td>\n",
       "      <td>-0.113005</td>\n",
       "      <td>-0.045958</td>\n",
       "      <td>0.043120</td>\n",
       "      <td>0.189523</td>\n",
       "      <td>0.014996</td>\n",
       "      <td>0.132203</td>\n",
       "      <td>-0.115639</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.413051</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>0.509778</td>\n",
       "      <td>-0.742404</td>\n",
       "      <td>0.208906</td>\n",
       "      <td>-0.287548</td>\n",
       "      <td>-0.517068</td>\n",
       "      <td>-0.135435</td>\n",
       "      <td>0.177530</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261332</td>\n",
       "      <td>-0.058636</td>\n",
       "      <td>-0.057109</td>\n",
       "      <td>0.020498</td>\n",
       "      <td>0.156747</td>\n",
       "      <td>-0.016800</td>\n",
       "      <td>0.138250</td>\n",
       "      <td>-0.179067</td>\n",
       "      <td>0.093943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0.810759</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>-0.641180</td>\n",
       "      <td>-0.734969</td>\n",
       "      <td>0.215605</td>\n",
       "      <td>0.102423</td>\n",
       "      <td>0.030575</td>\n",
       "      <td>-0.062887</td>\n",
       "      <td>-0.215410</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178152</td>\n",
       "      <td>-0.088722</td>\n",
       "      <td>0.041802</td>\n",
       "      <td>-0.085061</td>\n",
       "      <td>-0.032335</td>\n",
       "      <td>-0.062554</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>0.111075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0.766963</td>\n",
       "      <td>0.434614</td>\n",
       "      <td>-0.795144</td>\n",
       "      <td>-0.535864</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>0.216450</td>\n",
       "      <td>0.031868</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>-0.196559</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>0.122430</td>\n",
       "      <td>-0.140203</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>-0.008898</td>\n",
       "      <td>-0.021138</td>\n",
       "      <td>0.206536</td>\n",
       "      <td>0.029052</td>\n",
       "      <td>0.144971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.123206</td>\n",
       "      <td>-0.531445</td>\n",
       "      <td>-0.870132</td>\n",
       "      <td>0.426856</td>\n",
       "      <td>-0.102013</td>\n",
       "      <td>0.108357</td>\n",
       "      <td>-0.377542</td>\n",
       "      <td>-0.122371</td>\n",
       "      <td>-0.283178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230259</td>\n",
       "      <td>-0.027055</td>\n",
       "      <td>0.109464</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.226541</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.051280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0.910977</td>\n",
       "      <td>0.535527</td>\n",
       "      <td>-0.670594</td>\n",
       "      <td>-0.157994</td>\n",
       "      <td>0.485442</td>\n",
       "      <td>0.146686</td>\n",
       "      <td>-0.043265</td>\n",
       "      <td>-0.124458</td>\n",
       "      <td>-0.097262</td>\n",
       "      <td>-0.057348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>-0.080578</td>\n",
       "      <td>-0.165116</td>\n",
       "      <td>0.013067</td>\n",
       "      <td>-0.026326</td>\n",
       "      <td>0.203659</td>\n",
       "      <td>0.071344</td>\n",
       "      <td>0.045116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0.970835</td>\n",
       "      <td>0.345382</td>\n",
       "      <td>-0.327765</td>\n",
       "      <td>-0.459217</td>\n",
       "      <td>0.295382</td>\n",
       "      <td>0.153864</td>\n",
       "      <td>-0.162170</td>\n",
       "      <td>-0.003182</td>\n",
       "      <td>-0.582055</td>\n",
       "      <td>0.204127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130219</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>0.088295</td>\n",
       "      <td>-0.172877</td>\n",
       "      <td>0.053612</td>\n",
       "      <td>-0.123709</td>\n",
       "      <td>0.163031</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0   -0.692918  0.685098 -0.364534 -0.295984 -0.199671 -0.119706 -0.064272   \n",
       "1   -0.753818  0.647446 -0.444903 -0.416003 -0.273987 -0.100499 -0.143152   \n",
       "2   -0.655107  0.681262 -0.289378 -0.299772  0.038593 -0.109086 -0.244566   \n",
       "3   -0.435551 -0.059835  0.440370 -0.898822  0.513097 -0.349614 -0.591225   \n",
       "4   -0.413051 -0.004898  0.509778 -0.742404  0.208906 -0.287548 -0.517068   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "675  0.810759  0.264550 -0.641180 -0.734969  0.215605  0.102423  0.030575   \n",
       "676  0.766963  0.434614 -0.795144 -0.535864 -0.001188  0.216450  0.031868   \n",
       "677  0.864723  0.123206 -0.531445 -0.870132  0.426856 -0.102013  0.108357   \n",
       "678  0.910977  0.535527 -0.670594 -0.157994  0.485442  0.146686 -0.043265   \n",
       "679  0.970835  0.345382 -0.327765 -0.459217  0.295382  0.153864 -0.162170   \n",
       "\n",
       "          PC8       PC9      PC10  ...      PC17      PC18      PC19  \\\n",
       "0    0.107715  0.010683  0.011549  ...  0.027896 -0.120267 -0.123190   \n",
       "1    0.152431 -0.033701  0.082542  ...  0.128479  0.002374 -0.144448   \n",
       "2    0.177346 -0.090378  0.015818  ...  0.245304 -0.059516 -0.255164   \n",
       "3   -0.109355  0.091131  0.045074  ...  0.350818 -0.113005 -0.045958   \n",
       "4   -0.135435  0.177530  0.000200  ...  0.261332 -0.058636 -0.057109   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "675 -0.062887 -0.215410 -0.005058  ...  0.178152 -0.088722  0.041802   \n",
       "676  0.053700 -0.196559  0.013821  ... -0.001574  0.122430 -0.140203   \n",
       "677 -0.377542 -0.122371 -0.283178  ...  0.230259 -0.027055  0.109464   \n",
       "678 -0.124458 -0.097262 -0.057348  ...  0.039400  0.021273 -0.080578   \n",
       "679 -0.003182 -0.582055  0.204127  ...  0.130219  0.038588  0.088295   \n",
       "\n",
       "         PC20      PC21      PC22      PC23      PC24      PC25  Hired3C  \n",
       "0   -0.279014 -0.030345 -0.031081  0.040511 -0.006124 -0.035574        1  \n",
       "1   -0.251815  0.100197 -0.039564  0.097527 -0.033825  0.016212        1  \n",
       "2   -0.140905 -0.003871  0.092548  0.113161  0.052736 -0.089075        1  \n",
       "3    0.043120  0.189523  0.014996  0.132203 -0.115639  0.005501        1  \n",
       "4    0.020498  0.156747 -0.016800  0.138250 -0.179067  0.093943        1  \n",
       "..        ...       ...       ...       ...       ...       ...      ...  \n",
       "675 -0.085061 -0.032335 -0.062554  0.073359 -0.001346  0.111075        0  \n",
       "676 -0.142817 -0.008898 -0.021138  0.206536  0.029052  0.144971        0  \n",
       "677 -0.146216  0.038154  0.007096  0.226541  0.008123  0.051280        0  \n",
       "678 -0.165116  0.013067 -0.026326  0.203659  0.071344  0.045116        0  \n",
       "679 -0.172877  0.053612 -0.123709  0.163031  0.001659  0.143555        0  \n",
       "\n",
       "[680 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = pd.concat([principalDf, prosody_data[['Hired3C']]], axis = 1)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pressed-basics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_exp_var = []\n",
    "var_exp = 0\n",
    "for i in pca.explained_variance_ratio_:\n",
    "    var_exp += i\n",
    "    cum_exp_var.append(var_exp)\n",
    "\n",
    "# Plot cumulative explained variance for all PCs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.bar(range(1,26), cum_exp_var)\n",
    "ax.set_xlabel('# Principal Components')\n",
    "ax.set_ylabel('% Cumulative Variance Explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "center-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateModel( X_train, X_test, y_train, y_test):\n",
    "    y_21_test = []\n",
    "    for i in y_test:\n",
    "        if(i == 2):\n",
    "            y_21_test.append(2)\n",
    "        else:\n",
    "            y_21_test.append(1)\n",
    "\n",
    "    y_21_train = []\n",
    "    for i in y_train:\n",
    "        if(i == 2):\n",
    "            y_21_train.append(2)\n",
    "        else:\n",
    "            y_21_train.append(1)\n",
    "\n",
    "    X = X_train\n",
    "    y = y_21_train\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.score(X, y)\n",
    "    #AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    y_21_predict=clf.predict(X_test)\n",
    "    target_names = ['class 2', 'class 1']      \n",
    "    print(classification_report(y_21_test,y_21_predict,target_names=target_names, zero_division=0))\n",
    "\n",
    "    X_10_train = X_train.loc[y_train!= 2]\n",
    "    X_10_test = X_test.loc[y_test!= 2]\n",
    "\n",
    "    #select from y_train where y value is not 2\n",
    "    y_10_train= []\n",
    "    for i in y_train:\n",
    "        if(i != 2):\n",
    "            y_10_train.append(i)\n",
    "\n",
    "    #select from y_test where y value is not 2\n",
    "    y_10_test = []\n",
    "    for i in y_test:\n",
    "        if(i != 2):\n",
    "            y_10_test.append(i)\n",
    "\n",
    "    X = X_10_train\n",
    "    y = y_10_train\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.score(X, y)\n",
    "    y_10_predict = clf.predict(X_10_test)\n",
    "    target_names = ['class 1', 'class 0']      \n",
    "    print(classification_report(y_10_test,y_10_predict ,target_names=target_names, zero_division=0))\n",
    "\n",
    "    y1=list(y_test)\n",
    "    y2=list(y_21_predict)\n",
    "    y3=list(y_10_predict)\n",
    "    combined_Predictions = []\n",
    "    k=0\n",
    "    for (i,j) in zip(y1,y2):\n",
    "        if(i!=2):\n",
    "            combined_Predictions.append(y3[k])\n",
    "            k=k+1\n",
    "        else:\n",
    "            combined_Predictions.append(j)\n",
    "    #target_names = ['class 0', 'class 1', 'class 2']\n",
    "    #print(classification_report(y_test,combined_Predictions ,target_names=target_names, zero_division=0))\n",
    "    acc = accuracy_score(y_test, combined_Predictions, normalize=True)\n",
    "\n",
    "    return acc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eight-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       0.98      1.00      0.99        46\n",
      "     class 1       1.00      0.95      0.98        22\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.99      0.98      0.98        68\n",
      "weighted avg       0.99      0.99      0.99        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        46\n",
      "   macro avg       1.00      1.00      1.00        46\n",
      "weighted avg       1.00      1.00      1.00        46\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        46\n",
      "     class 1       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.96      1.00      0.98        23\n",
      "     class 0       1.00      0.96      0.98        23\n",
      "\n",
      "    accuracy                           0.98        46\n",
      "   macro avg       0.98      0.98      0.98        46\n",
      "weighted avg       0.98      0.98      0.98        46\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        46\n",
      "     class 1       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        46\n",
      "   macro avg       1.00      1.00      1.00        46\n",
      "weighted avg       1.00      1.00      1.00        46\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        46\n",
      "     class 1       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.95      0.91      0.93        23\n",
      "     class 0       0.92      0.96      0.94        23\n",
      "\n",
      "    accuracy                           0.93        46\n",
      "   macro avg       0.94      0.93      0.93        46\n",
      "weighted avg       0.94      0.93      0.93        46\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        46\n",
      "     class 1       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        46\n",
      "   macro avg       1.00      1.00      1.00        46\n",
      "weighted avg       1.00      1.00      1.00        46\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        45\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        45\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        45\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       0.98      1.00      0.99        45\n",
      "     class 1       1.00      0.96      0.98        23\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.99      0.98      0.98        68\n",
      "weighted avg       0.99      0.99      0.99        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 2       1.00      1.00      1.00        45\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       1.00      1.00      1.00        23\n",
      "     class 0       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "[98.52941176470588, 98.52941176470588, 100.0, 95.58823529411765, 100.0, 100.0, 100.0, 100.0, 98.52941176470588, 100.0]\n",
      "CV Score: 99.12% (+/- 1.35%)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10,shuffle=True, random_state=12)\n",
    "cvscores =[]\n",
    "X= finalDf[['PC1', 'PC2','PC3', 'PC4','PC5', 'PC6','PC7', 'PC8','PC9', 'PC10','PC11', 'PC12','PC13', 'PC14','PC15', 'PC16','PC17', 'PC18','PC19', 'PC20','PC21', 'PC22','PC23', 'PC24','PC25']]\n",
    "y=finalDf['Hired3C']\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    scores = CreateModel( X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    \n",
    "    cvscores.append(scores*100)\n",
    "print(cvscores ) \n",
    "print(\"CV Score: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-sharp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-dylan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
